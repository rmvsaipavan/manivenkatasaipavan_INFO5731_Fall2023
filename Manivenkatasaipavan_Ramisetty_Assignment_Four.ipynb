{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmvsaipavan/manivenkatasaipavan_INFO5731_Fall2023/blob/main/Manivenkatasaipavan_Ramisetty_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "0a164f51-2f57-495f-9914-b1af46d19527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '0.040*\"movie\" + 0.028*\"oppenheimer\" + 0.013*\"hiroshima\" + 0.013*\"time\" + 0.010*\"would\"')\n",
            "(1, '0.024*\"oppenheimer\" + 0.020*\"really\" + 0.018*\"nolan\" + 0.016*\"one\" + 0.016*\"scene\"')\n",
            "(2, '0.041*\"movie\" + 0.030*\"story\" + 0.022*\"maybe\" + 0.018*\"get\" + 0.018*\"one\"')\n",
            "(3, '0.032*\"film\" + 0.028*\"nolan\" + 0.020*\"movie\" + 0.018*\"oppenheimer\" + 0.012*\"like\"')\n",
            "(4, '0.032*\"film\" + 0.020*\"long\" + 0.020*\"nolan\" + 0.016*\"one\" + 0.012*\"watched\"')\n",
            "(5, '0.019*\"time\" + 0.016*\"really\" + 0.016*\"great\" + 0.016*\"oppenheimer\" + 0.013*\"best\"')\n",
            "(6, '0.071*\"film\" + 0.030*\"nolan\" + 0.025*\"also\" + 0.025*\"oppenheimer\" + 0.020*\"unsatisfying\"')\n",
            "(7, '0.030*\"oppenheimer\" + 0.026*\"film\" + 0.020*\"one\" + 0.018*\"feel\" + 0.018*\"time\"')\n",
            "(8, '0.059*\"movie\" + 0.024*\"nolan\" + 0.022*\"one\" + 0.016*\"like\" + 0.012*\"also\"')\n",
            "(9, '0.025*\"oppenheimer\" + 0.012*\"nolan\" + 0.012*\"film\" + 0.009*\"man\" + 0.007*\"robert\"')\n",
            "\n",
            "Cluster 8:\n",
            "Number of documents: 2000\n",
            "Sample document:\n",
            "This movie is just... wow! I don't think I have ever felt like this watching a movie! Its like a blend of being sad but also scared! I read that Christopher Nolan said it kind of had themes of horror, and watching the movie i think I knew what he meant! Very few movies can make you feel quite like this one can!Nolan once again shows he is an expertly craftsman in filmmaking! This stands as perhaps one of his more humble movies but also one of his greatest! Reminds me of his earlier movies!The cast is also AMAZING with Cillian Murphy delivering the performance of his carrer as Oppenheimer, esentially becoming him, and pretty much securing himself an Oscar nomination for best lead actor! Robert Downey Junior also gives one of his best performances, reminding us all that despite 10 years as Iron man, he can still act!The soundtrack, sound and editing is also masterfull and further creates a cinematic experience like no other!Overall an esential viewing experience about historic events that still remains very relevant to this day! One of my favorite Nolan movies!\n",
            "\n",
            "Cluster 5:\n",
            "Number of documents: 1200\n",
            "Sample document:\n",
            "You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\n",
            "\n",
            "Cluster 7:\n",
            "Number of documents: 1200\n",
            "Sample document:\n",
            "One of the most anticipated films of the year for many people, myself included, Oppenheimer largely delivers. Much of it's great. I feel like I loved two of its three hours, and liked the other hour.... but it's that fact that stops me from adoring the entire thing. I know with Christopher Nolan's Dunkirk, that clicked on a second watch, so maybe Oppenheimer will need one too. That being said, I don't feel the need to rush out and see it again too soon, because it was a long and exhausting film.But in many ways, I can't deny it was an exceptionally well made one. It looks and sounds as amazing as you'd expect, feeling as though it accurately captures the time period it's set in, and containing amazing sound design and one of the year's best scores so far. Every performance is good to great, but the film belongs to Cillian Murphy, and I feel like he's the lead actor to beat at this stage, if we're talking (early) awards consideration.The film's at its best when it focuses on being a psychological thriller featuring a famous historical figure, and at one point, it even turns into a psychological horror film. There's one sequence in here involving a speech that's particularly terrifying. It also manages to have some very suspenseful moments, even though its story is commonly known history at this point.I did really feel the length in the final hour, though, and maybe I wish that final act had been more of an extended epilogue, rather than a whole third of the movie. I currently feel as though I would've loved Oppenheimer more had it been 2.5 hours instead of 3, but nothing about it was bad by any means; just a little patience testing (this is very subjective - I remember feeling like the similarly long Babylon totally justified its runtime, though others didn't feel that way).I'm left feeling like I watched a film that wasn't a slam dunk, but was incredible for more of its runtime than it wasn't. And that's still worth celebrating, and makes Oppenheimer worth seeing in cinemas for sure.\n",
            "\n",
            "Cluster 4:\n",
            "Number of documents: 1200\n",
            "Sample document:\n",
            "Oppenheimer might be the best film I watched in a long, long time.Very different than Nolan's recent films, especially the Sci-Fi ones, but shows that Nolan can master the Biopic/Drama genre just as well as he can any other genre he tried to tackle yet.The film is 3-hours long but goes through very quickly and enjoyably. Without spoiling anything, the film presents important and very relevant subjects, and doing so while being non-stop entertainment and a comprehensive character study and a study of our society on a very high pace.Without mentioning anything specific, there was one scene that caused almost every single person in the theatre to move nervously in the seats, non-stop for a long period of time, being one of the most intense scenes I ever watched in a movie and reminding me of the true power of the cinematic experience like no other movie did in recent years.The year is only half-way through but right now this is my top pick for the upcoming awards season. Picture, Writing, Directing, Acting, Score-- Oppenheimer is a winner on all fronts. A rare feat for filmmaking and a salient reminder that cinema is not dead.I highly recommend this film to everyone. Watched it once already, and going back to the theatre for at least a few more times soon.\n",
            "\n",
            "Cluster 3:\n",
            "Number of documents: 1200\n",
            "Sample document:\n",
            "Just came out of the theater and watching Oppenheimer was such a great experience. I know many people will criticize the movie for some historical accuracy absence but I think Christopher Nolan has made this complicated man's story compelling, engaging, and simple to understand. The actors are phenomenal. Apart from the main leads, Robert Downey has probably done one of his finest work. His expressions, timing, delivery... Everything was on par. The cinematography has been crafted beautifully. I adored and enjoyed the whole three hours with ease and delight. This is the first attempt of Christopher Nolan at biographies and I think we should expect more of his work from this genre since it's not only entertaining but also sparks an interest to know history more. I have read the book earlier so I went to watch it with a little bit of knowledge and still enjoyed the film. I wish I could tell Cillian Murphy in person how stunning his screen presence has been throughout. Hopefully, this movie wins the awards like it deserves.\n",
            "\n",
            "Cluster 9:\n",
            "Number of documents: 800\n",
            "Sample document:\n",
            "\"Oppenheimer\" is a biographical thriller film written and directed by Christopher Nolan (\"The Dark Knight trilogy\", \"Inception\", \"Interstellar\", \"Dunkirk\"), based on the biography \"American Prometheus\" by Kai Bird and Martin J. Sherwin. Starring Cillian Murphy in the lead role, in addition to Matt Damon, Robert Downey Jr, Emily Blunt, and Florence Pugh, it subverts the usual biopic formula to create a brilliantly layered examination of a man throughout all of his incredible accomplishments and fundamental flaws.During the height of the Second World War, theoretical physicist J. Robert Oppenheimer (Cillian Murphy) is recruited by the United States government to oversee the \"Manhattan Project\", a top secret operation intended to develop the world's first nuclear weapons. After becoming acquainted with the project's director Major General Leslie Groves (Matt Damon), Oppenheimer and the General come to an agreement that the best place to carry out such an undertaking is the vast desert of Los Alamos, New Mexico. As numerous other scientists and their families are brought in to this discreet location, Oppenheimer works tirelessly around the clock to build this weapon of mass destruction before the Nazis can devise their own. With the War raging and personal troubles mounting, Oppenheimer continues to push himself to his utmost limits, but soon suffers the consequences of his dedication.On August 6th, 1945, the atomic bomb \"Little Boy\" was dropped on the Japanese city of Hiroshima, making it the first time a nuclear weapon was used in an act of war. The dropping of this bomb and \"Fat Man\" in Nagasaki three days later was what essentially brought an end to World War II, and with it, began a frightening new era known as the \"Atomic Age\". To this day, it remains a contentious topic of discussion among many as to whether the ethical ramifications of these bombings are justified by what subsequently resulted from it. The one man whom most people pin all the blame on is J. Robert Oppenheimer, whose key role in the development of these weapons led to him being credited as \"the father of the atomic bomb\", a label which he carried as a heavy burden for the rest of his life. In Christopher Nolan's biopic \"Oppenheimer\", we are treated to an intricately structured and uniquely tragic analysis of this complex man's legacy and how it still affects everyone several decades later.In true Christopher Nolan fashion, the story is not told as a conventional biopic but rather as a fragmented, non-sequential series of highlights pertaining to the title subject's life. When we are first introduced to J. Robert Oppenheimer, we see that he is an incredibly intelligent man whose sheer commitment to his craft earns him the utmost respect of many of his peers, even as a young student. However, almost immediately after, the film cuts to him on trial for allegedly having ties to communism, an accusation which threatens to completely derail the positive reputation bestowed upon him. As the judicial committee interrogates him with hard hitting questions, Oppenheimer is haunted by the errors of his own judgement, shown to the audience through flashbacks of varying length to pivotal times in his life. These scenes range from his fractured relationship with his wife to the remorse he has for placing his trust in the wrong people. This gives the viewer a first person perspective of what Oppenheimer's mind must have been processing during this intense period of his life, as he contemplates the very real possibility of having all of his hard work mitigated by these powerful government officials. It's hard to imagine any other director trying to convey so much information to their audience in such a fashion, but Nolan manages to work his magic in the best way possible, always striking the perfect balance of showcasing the triumphant rise and tragic fall of an imperfect man.Another notable thing about Nolan's direction is his resourcefulness in the way he handles certain important scenes. One moment, which I won't speak about in too much detail, literally had me on the edge of my seat as we watch Oppenheimer and the rest of the scientists test out the prototype bombs with each explosion proving to be bigger than the last. Since Nolan has been vocal about his dislike of using CGI in his movies, he instead opts for more practical methods of showing the increasing power of these bombs. By reminding the audience that the Nazis could very well be working on their own weapon of mass destruction, there is a real sense of urgency flowing throughout these scenes, giving the scientists all the more reason to work even faster to beat the enemy at their own game. As each bomb explodes, it can be likened to a ticking clock, with each blast representing progression towards the end goal of perfecting the ultimate weapon. The creative use of editing during these scenes keeps things moving at a brisk pace, something especially necessary considering the film's three hour runtime. Nolan previously exhibited a similar method in 2017's \"Dunkirk\", which utilised the film's score in a clever way to show the audience how time is truly of the essence. Once again, Nolan has found a clever way around taking the easy route of using CG effects to tell a story, and keeps viewers on their toes by use of good old fashioned directorial proficiency.For what can only be described as the performance of his career, Cillian Murphy brings everything necessary to the role of J. Robert Oppenheimer, a man so complex that I can't imagine the amount of pressure there was to play him this effectively. The combined efforts of both Murphy's acting and Nolan's direction help make Oppenheimer one of the most fascinating individuals of the 20th century. This is not a man who can be viewed simply at face value, as there are so many layers to his character that it bears an in-depth exploration that only a movie like this can accomplish. The film paints Oppenheimer as neither a hero nor a villain, but rather a complicated man whose human qualities undermine what he will be remembered for in the history books. Murphy approaches him like that of a Shakespearian figure, rife with flaws, haughtiness, and a sense of hubris that ends up sealing his inevitable fate. One scene may have you admiring his remarkable talents in the field of nuclear physics while another might cause you to hate him for his unfaithfulness to his family. He can be viewed simultaneously as a martyr and a scapegoat for the way in which he helped bring an end to the deadliest global conflict in history, while consequently ushering in something even worse.The rest of the film's cast all did a fantastic job as well, with the standouts being Matt Damon, Robert Downey Jr, Emily Blunt, and Florence Pugh. Damon's take on Major General Leslie Groves is more than simply that of a stock military character but rather an important figure who seizes the opportunity to use Oppenheimer's talents to his advantage. We watch as Groves forms an unlikely alliance with the physicist, often questioning the ramifications of the theoretical nature in experimenting with nuclear power. Groves's ignorance to Oppenheimer's extensive scientific knowledge allows the audience to learn along with him when it is explained in basic detail. To that effect, he provides an important third party perspective to Oppenheimer's achievements.It's also great to see Robert Downey Jr shine as Lewis Strauss, which is not only his best post-MCU role but one of his best roles in general. Strauss is a man who is not viewed favourably by history due to his role in exposing Oppenheimer's ties to communism. He holds such a grudge against Oppenheimer that you can practically consider him the true villain of this story. Downey takes every opportunity to show Strauss's two-faced nature, biding his time for the right moment to strip Oppenheimer from the record books and damage his reputation. Reportedly, Downey considers this his best role to date, and it definitely seems like he is putting everything he has into his performance.Emily Blunt and Florence Pugh also contributed significantly as Kitty Oppenheimer and Jean Tatlock, respectively. Each of these two women represent something significant in Oppenheimer's life, with Kitty being who he should be with and Jean being who he personally wants to be with. This draws parallels to that of Oppenheimer choosing between acting on instinct or acting on intellect when assisting in the construction of the bomb, which again reminds the audience of his flawed human qualities. It can be difficult to give up following your heart but when the fate of the world rests on your pragmatic decision making, sometimes you have no other choice.As a biopic and a Christopher Nolan film, \"Oppenheimer\" exceeds virtually all expectations to become one of the very best in both fields. There are few films that are able to tackle such subject matter in this much detail while also remaining entertaining the whole way through. I guess sometimes all it takes is one brave, risk-taking filmmaker to prove that this really is a possible task. We need more films like this to inspire thoughtful, creative discussion and it is comforting to know that someone like Nolan is here to help keep them in the mainstream. After all, it's a tough job, but someone's got to do it for us.I rate it a perfect 10/10.\n",
            "\n",
            "Cluster 0:\n",
            "Number of documents: 800\n",
            "Sample document:\n",
            "I was familiar with the Manhattan project and the social and political aftermath, so \"Oppenheimer\" was an excursion into known territory.Is it Nolan's finest movie to date? Not really. Because I know he can do even better. Does it touch upon greatness? Yes, a couple of times!THE GREAT +++Cillian Murphy gives one of the most surprising leading man performances in ages and just might win an Oscar for his excellent portrayal of Robert Oppenheimer. He fully transforms into the highly complex and increasingly conflicted man, his eyes ooze tension, his voice is on point, and his demeanour is congruent.The entire 2nd act (the building of the Manhattan Project's Los Alamos laboratory and the eventual detonation of the first atomic bomb) is the best part of the movie.Oppenheimer's shaken and stirred speech after the Trinity test is arguably the most engrossing and immersive movie-making of this summer.Great Hollywood actors and actresses galore.Many poignant time jump edits.The scene with Oppenheimer and president Truman in the oval office.Hair and make-up department convincingly depicts Oppenheimer (and some of the rest) from student years to the last stages of their lives.In (surprisingly) typical Nolan fashion, the very ending is a satisfying (if slightly fizzled by the 3rd act) \"twist\" that was being built up throughout the movie.THE NOT SO GREAT ---At least 60% of the movie is a dissapointing bait-and-switch. It focuses way too much time on Oppenheimer's pre-Trinity political activity and post-Trinity kangaroo trial without a satisfactory reason to do so. The 1st and especially the overly long 3rd act of the movie needed tighter editing and the movie would have been better for it. The build-up to the powerful ending of the movie could easily be slightly modified and still pack the same if not an even bigger, much needed gut-wrenching punch.The movie is overly dense because it tries to jam-pack too much of Oppie's pre- and post-Trinity life into one movie. Nolan adds a plethora of new characters we can just barely remember and his time jump edits are sometimes unnecessary and hard to follow.Not nearly enough of the movie is about the actual technical feat of building the first atomic bomb.\n",
            "\n",
            "Cluster 6:\n",
            "Number of documents: 800\n",
            "Sample document:\n",
            "I'm still collecting my thoughts after experiencing this film, Cillian Murphy might as well start clearing a space on his mantle for the Best Actor Oscar.This film is a masterclass in weaving narratives and different time periods while exploring the profound depths of a man whose actions altered the world's trajectory forever, for better or worse. Nolan brings us into the complexities of Oppenheimer, and all the moral conflicts stirring within him.Murphy's portrayal is so riveting that the long run-time became an afterthought. Robert Downey Jr also offers a great performance and Nolan's push and pull with how he uses sound design throughout is the cherry on top.Some viewers might need a brief refresher on WWII and Cold War history, but any film lover should be happy to willingly lose themselves in this film for hours on end.\n",
            "\n",
            "Cluster 2:\n",
            "Number of documents: 400\n",
            "Sample document:\n",
            "I'm a big Nolan fan. Maybe this one just wasn't for me.This movie was promoted as the story of the invention of the bomb. We were told we should see it on the biggest screen. Go out of your way for an IMAX 70mm projection if you can, or at least get a regular 70mm or Laser IMAX.It turns out, that while Nolan's visuals still look good in this movie, there's nothing breathtaking that warrants those formats. It's like a 1 Michelin star, if your local theater has those formats, might as well go for it, but that's probably true for any movie.It also turns out that this is mostly the story of political vendettas and the marketing of bombs exploding had little to do with the movie, the whole trailer is really just one scene. I guess there wasn't a more interesting way to approach the Los Alamos part of the story. What could he do, focus on the scientific challenges? Not really the best source for a story.We get the review of Oppenheimer for a security clearance, which leads to his retelling of his history in flashbacks, which is why the story of creating the bomb is told as well as his personal relationships (I guess because she was a communist party member, the character played by Pugh is naked all the time, not sure why that was necessary) while at the same time we get a much shorter story of the confirmation hearings of Strauss, with a few flashbacks to a couple of meetings he was involved in with Oppenheimer .It's a political drama, with not much drama. There isn't enough emotional connection to the characters to care about it. Will Oppenheimer lose his security clearance? Will Strauss get served by karma at his cabinet confirmation hearing? Who cares? If you're not invested in any of the characters, why would you care about what happens to them in the end?Maybe this is the best one can do with the source material. Maybe the mistake was wanting to tell that particular story. Maybe better writing and staying away from the gimmick of two timelines of flashbacks intersecting could've told the story better. Maybe it's time for Nolan to let go of those gimmicks in his movies and trust his subject matter.Unlike previous Nolan movies, I won't be thinking about this movie much or wanting to watch it again to understand it better. There's just nothing else in there.For me, this isn't one of his top movies. Technically it's great, the acting is also great, the writing and editing just isn't there.Also, IMAX is great, but not every movie benefits from it. This one really didn't. Maybe if Nolan wasn't so focused on the technical side he'd be able to get me emotionally invested in this story.Edit:I had to come back and add a comparable movie that does a much better job and shows you can tell this story with real tension.The Imitation Game.A movie about a genius trying to solve a problem during the same war. That topic is just as science heavy and the movie goes into his personal struggles, relationships and the personal consequences he suffered because of the politics of the time.The tension is focused on the task of breaking the German code. Not on what will happen to the lead characters years later. It's a lot easier to be invested in that (even though we know who won that war) vs. The results of some political hearing.\n",
            "\n",
            "Cluster 1:\n",
            "Number of documents: 400\n",
            "Sample document:\n",
            "Okay, Nolan fans, get your fingers poised to downvote what I'm about to say. That's the only way I can understand the high rating for this film - thousands of devoted Nolan fans inflating the score. Because if you're honest, there's no way this mottled mess of a movie is an 8.9. Not in any sane universe.I've seen all of Nolan's films. Memento was a brilliant calling card for a young director and The Dark Knight elevated superhero movies to something amazing, gritty and crackling with verisimilitude. But Inception was a long slog of exposition, and Interstellar, while offering some good moments, also imploded under the weight of the writer-director's ego.Nolan likes to tackle big ideas. Dreams and outer space. Here, he delves into quantum physics, but it's relegated to a line, really, that Oppenheimer offers Kitty before marrying her. He explains quantum physics as mostly space in which particles have an \"attraction\" to one another... and then they hold hands.For the first two hours of Oppenheimer, I was lost in a blizzard of short, disparate scenes, constant musical score, actors chewing through endless dialog. You are never allowed to rest, never really sure where or when you are. Nolan offers only two cryptic title cards at the very beginning: 1. Fission and 2. Fusion. (Or maybe it was the other way around.) He separates the time period of public hearing, with Robert Downey Jr. As the main character, by desaturating to black and white. But other than the make-up used to age or de-age Cillian Murphy's Oppenheimer, you never really know quite where you are, or where it fits in sequentially or contextually into the story.There is the public hearing, and there is a closed hearing with a wolf pack of hungry prosecutors, and then there is some semblance of Oppenheimer's backstory - his love life, his gradual involvement in government, leading to a general (Matt Damon) for some reason hiring him to be the head of Los Alamos. All of this is mashed together, scenes never really more than a few seconds long before cutting to somewhere else, something else, often interpolated with macro shots of things fizzing and roiling and exploding. I assume that's supposed to be some sort of visual metaphor for the work Oppenheimer is doing, his theorizing and contemplating, but that's it, for any demonstration of the actually \"work\" Oppenheimer does, save one scene near the beginning where he inadvertently shatters a beaker in class and we hear he's terrible in a lab.Yet, without exaggeration, by an hour into the movie, we've been told at least ten times that Oppenheimer is brilliant, or a \"genius.\" We're just never shown why. And this is Nolan's chief sin - he is a teller, not a shower. A writer, not really a director.Take \"A Beautiful Mind\" for comparison. In that movie, director Ron Howard regularly visualizes the work of John Nash. He shows him, for instance, watching pigeons gather crumbs, and in his mind's eye he maps their pattern. Or on a window overlaying the view outside of some young men playing sports, he uses a piece of white pastel to draw a diagram of them. Film is a visual medium.Consider \"Schindler's List\" (or any Spielberg movie, really), and observe the blocking of the actors, the placement of the camera, all in service of telling the story visually. An actor may dominate the frame, or maybe have his back turned. Characters may move and create an entirely new frame (blocking). Their relationship to each other and to the camera help tell the story.In Nolan's filmmaking, where the camera goes is really arbitrary. And where he cuts the shot has to do with his writing, not the actors reaction or the blocking of the scene, so that the editing feels off, clunky, the shot moving off of an actor at the start of a reaction, or coming back with an actor already in motion. This is because Nolan is cutting for the script, relying on dialog to tell the entire story. Even plays have blocking.True, he decides to tell the story of the \"Father of the A-Bomb\" chiefly through these two hearings, the public one and the closed one, so there's going to be lots of talking. But then he doubles down on the talking - on the telling - even further. In one scene, Casey Affleck sits beside Oppenheimer in some room somewhere (I don't even know who Affleck was playing, really, it was very short) and while he's talking to Oppenheimer, Nolan cuts back and forth to another scene with Matt Damon on a train with Oppenheimer, and Damon is telling us about Affleck and who he is and what he wants.Character should be revealed through action. Not some other character explaining everything off to the side.There are a great many cameos in Nolan's film - it is \"star studded.\" But rather than the appearance of a name actor helping to clarify the character portrayed, they distract. At one point, a woman near me in the theater said \"Oh, look who that is,\" when Remi Malek appeared. We're focused on the actor and their previous roles, not the character.Everything in this movie, especially the first ninety minutes, bounced me off, like a stone skipping over water. The scenes are too short, the music never stops, there are too many characters, we're always changing time and place, I'm not really sure what's happening, famous faces keep popping up. And I don't understand why everything is so frantic.Once we get to about the halfway mark, and the Trinity project gets that infamous test, the movie sinks in a little. That's because Nolan finally slows things down, lets us exist somewhere in the film for a moment, lets us be immersed.After that, for the most part, he's back a it, whisking us from one quick scene to the next at a pace that tries so hard to be breathless and exciting and just ends up distracting and frustrating. Still, I felt more in tune for the second half of the film, because I could now sense the dilemma, the emotional conflict in Oppenheimer after Hiroshima and Nagasaki. In one of the best scenes in the film, Oppenheimer is giving a speech to toast the success of the American empire, but the room turns white, and rumbles, and woman's skin flays.And finally, another scene with Emily Blunt, as Kitty Oppenheimer, giving one of those wolf pack prosecutors a piece of her mind in the closed hearing, really steals the show. Blunt was truly an enjoyable part of this movie, though she had little screen time.There's not much else to say. I feel like I just listened to some hyperactive child try to tell me a story that I thought I already knew, but became unnecessarily convoluted in the telling. I didn't really learn anything new, not about the physics of the A-Bomb, nor did I really get a sense of the McCarthyism of the era; they were just after Oppenheimer for no real reason I could grasp, until very close to the end, apparently it was all because he had some reservations about using the H-Bomb.Nolan tries a twist, too, holding back on a brief conversation between Oppenheimer and Einstein by a pond. Because several of Nolan's films have had a big twist, this one felt kind of paltry as twists go, but drove home the underlying grief and sadness of the whole A-Bomb project, and what it meant for the world.6.5/10.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "# Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df['clean_tokens'] = df['clean_text'].apply(preprocess_text)\n",
        "\n",
        "# Create a dictionary and a corpus\n",
        "dictionary = Dictionary(df['clean_tokens'])\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in df['clean_tokens']]\n",
        "\n",
        "# Build the LDA model\n",
        "num_topics = 10\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the top 10 topics and their keywords\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# Assign each document to a topic\n",
        "df['topic'] = df['clean_tokens'].apply(lambda x: lda_model[dictionary.doc2bow(x)])\n",
        "\n",
        "# Extract the dominant topic for each document\n",
        "df['dominant_topic'] = df['topic'].apply(lambda x: max(x, key=lambda item: item[1])[0] if x else None)\n",
        "\n",
        "# Print the top 10 clusters and summarize the topic for each cluster\n",
        "top_clusters = df['dominant_topic'].value_counts().head(10)\n",
        "\n",
        "for cluster_id, count in top_clusters.items():\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    documents_in_cluster = df[df['dominant_topic'] == cluster_id]['clean_text']\n",
        "\n",
        "    print(f\"Number of documents: {count}\")\n",
        "    print(\"Sample document:\")\n",
        "    print(documents_in_cluster.iloc[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "3ce1e264-8732-4679-f343-eeb6b80f3e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Naive Bayes Classifier:\n",
            "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
            "Mean Cross-Validation Accuracy: 1.0\n",
            "\n",
            "Performance Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n",
            "\n",
            "Logistic Regression Classifier:\n",
            "Cross-Validation Scores: [1. 1. 1. 1. 1.]\n",
            "Mean Cross-Validation Accuracy: 1.0\n",
            "\n",
            "Performance Metrics:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(\n",
        "    df['clean_text'], df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature extraction: Convert text data to numerical features using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "# Function to train and evaluate a classifier\n",
        "def evaluate_classifier(classifier, X_train, y_train, X_test, y_test):\n",
        "    # Cross-validation (5-fold)\n",
        "    cv_scores = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    print(\"Cross-Validation Scores:\", cv_scores)\n",
        "    print(\"Mean Cross-Validation Accuracy:\", cv_scores.mean())\n",
        "\n",
        "    # Train the classifier on the entire training set\n",
        "    classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions on the test set\n",
        "    predictions = classifier.predict(X_test)\n",
        "\n",
        "    # Evaluate performance metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions, average='weighted')\n",
        "    recall = recall_score(y_test, predictions, average='weighted')\n",
        "    f1 = f1_score(y_test, predictions, average='weighted')\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(\"\\nPerformance Metrics:\")\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "# Build and evaluate the Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "print(\"\\nNaive Bayes Classifier:\")\n",
        "evaluate_classifier(nb_classifier, X_train, train_labels, X_test, test_labels)\n",
        "\n",
        "# Build and evaluate the Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression(max_iter=1000)\n",
        "print(\"\\nLogistic Regression Classifier:\")\n",
        "evaluate_classifier(lr_classifier, X_train, train_labels, X_test, test_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvMKJjIXS5G",
        "outputId": "0c9e367e-9053-43a3-be55-4bcdc9312063"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 1576962754.884255\n",
            "R^2 Score: 0.794407341710365\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the training dataset\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Assuming the target variable is 'SalePrice' and you want to use some features for prediction\n",
        "features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "\n",
        "# Select features and target variable\n",
        "X = train_df[features]\n",
        "y = train_df['SalePrice']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "\n",
        "# Visualize predicted vs actual prices\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Actual Prices vs Predicted Prices')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **pre-trained Large Language Model (LLM) from the Hugging Face Repository** for your specific task using the data collected in Assignment 3. After creating an account on Hugging Face (https://huggingface.co/), choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any Meta based text analysis model. Provide a brief description of the selected LLM, including its original sources, significant parameters, and any task-specific fine-tuning if applied.\n",
        "\n",
        "Perform a detailed analysis of the LLM's performance on your task, including key metrics, strengths, and limitations. Additionally, discuss any challenges encountered during the implementation and potential strategies for improvement. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNLA_fnfTFSR",
        "outputId": "e0364565-e913-42b7-b47e-166a447f0321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2022.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: torch in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (1.10.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2.7.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2022.2.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (2.1.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "['positive' 'negative']\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "# Check unique values in the 'sentiment' column\n",
        "print(df['sentiment'].unique())\n",
        "\n",
        "# Convert sentiment labels to numerical values if needed\n",
        "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}  # Adjust as per your dataset\n",
        "df['sentiment'] = df['sentiment'].map(label_mapping)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming 3 classes (positive, negative, neutral)\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(list(train_df['clean_text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(list(test_df['clean_text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Convert labels to torch tensors\n",
        "train_labels = torch.tensor(list(train_df['sentiment']))\n",
        "test_labels = torch.tensor(list(test_df['sentiment']))\n",
        "\n",
        "# Print unique values in the labels for verification\n",
        "print(train_labels.unique())\n",
        "print(test_labels.unique())\n",
        "\n",
        "# Fine-tune the model with gradient accumulation\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "accumulation_steps = 2  # Accumulate gradients over 2 steps\n",
        "model.train()\n",
        "for epoch in range(3):  # You can adjust the number of epochs\n",
        "    total_loss = 0\n",
        "    for step in range(0, len(train_labels), accumulation_steps):\n",
        "        batch_labels = train_labels[step:step+accumulation_steps]\n",
        "        batch_encodings = {key: value[step:step+accumulation_steps] for key, value in train_encodings.items()}\n",
        "\n",
        "        outputs = model(**batch_encodings, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_labels)}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels.numpy(), predictions.numpy())\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg9I5l88TFSR",
        "outputId": "c5c928bc-0fec-4a1e-ddcf-65f70486ab3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: requests in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2022.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: torch in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: sympy in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2.7.1)\n",
            "Requirement already satisfied: fsspec in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (2022.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pandas) (1.26.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "['positive' 'negative']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0, 1])\n",
            "tensor([0, 1])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install pandas\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('annotated_reviews.csv')\n",
        "\n",
        "# Check unique values in the 'sentiment' column\n",
        "print(df['sentiment'].unique())\n",
        "\n",
        "# Convert sentiment labels to numerical values if needed\n",
        "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2}  # Adjust as per your dataset\n",
        "df['sentiment'] = df['sentiment'].map(label_mapping)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a pre-trained model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)  # Assuming 3 classes (positive, negative, neutral)\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(list(train_df['clean_text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "test_encodings = tokenizer(list(test_df['clean_text']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Convert labels to torch tensors\n",
        "train_labels = torch.tensor(list(train_df['sentiment']))\n",
        "test_labels = torch.tensor(list(test_df['sentiment']))\n",
        "\n",
        "# Print unique values in the labels for verification\n",
        "print(train_labels.unique())\n",
        "print(test_labels.unique())\n",
        "\n",
        "# Fine-tune the model with gradient accumulation\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "accumulation_steps = 2  # Accumulate gradients over 2 steps\n",
        "model.train()\n",
        "for epoch in range(3):  # You can adjust the number of epochs\n",
        "    total_loss = 0\n",
        "    for step in range(0, len(train_labels), accumulation_steps):\n",
        "        batch_labels = train_labels[step:step+accumulation_steps]\n",
        "        batch_encodings = {key: value[step:step+accumulation_steps] for key, value in train_encodings.items()}\n",
        "\n",
        "        outputs = model(**batch_encodings, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {total_loss / len(train_labels)}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**test_encodings)\n",
        "    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Calculate key metrics\n",
        "accuracy = accuracy_score(test_labels.numpy(), predictions.numpy())\n",
        "classification_rep = classification_report(test_labels.numpy(), predictions.numpy())\n",
        "\n",
        "# Print key metrics and classification report\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "# Perform a detailed analysis\n",
        "# Strengths:\n",
        "# - Pre-trained LLMs capture semantic relationships in language.\n",
        "# - Transfer learning allows effective use of pre-existing knowledge.\n",
        "# - Transformers handle long-range dependencies well.\n",
        "\n",
        "# Limitations:\n",
        "# - Fine-tuning might require a large labeled dataset.\n",
        "# - Limited interpretability of the model's decisions.\n",
        "# - Domain-specific nuances might not be well-captured.\n",
        "\n",
        "# Challenges:\n",
        "# - Memory constraints during training.\n",
        "\n",
        "# Potential Strategies for Improvement:\n",
        "# - Experiment with different models and hyperparameters.\n",
        "# - Increase the size of the labeled dataset for fine-tuning.\n",
        "# - Explore domain-specific pre-training or fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJkhzFDjTFSS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}