{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmvsaipavan/manivenkatasaipavan_INFO5731_Fall2023/blob/main/Ramisetty_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LmNR3kw9-pa"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\n",
        "Research Question:\n",
        "How does the pricing of books on e-commerce platforms vary based on their genre, average user rating, and publication date?\n",
        "\n",
        "Data Needed for Analysis:\n",
        "\n",
        "Book Details: Title, Genre, Average User Rating, Publication Date.\n",
        "Price: The price of the book.\n",
        "Platform Information: Information about the e-commerce platform where the books are listed (e.g., Amazon, eBay).\n",
        "Steps for Data Collection:\n",
        "\n",
        "Identify the Source:\n",
        "\n",
        "Choose an e-commerce platform (e.g., Amazon, eBay) that provides access to book listings.\n",
        "Set up Scraping Environment:\n",
        "\n",
        "Use Python and libraries like requests, bs4, and csv to create a scraping script.\n",
        "Access the Pages:\n",
        "\n",
        "Use a base URL and loop through the pages of book listings to collect data. (Similar to the code you provided)\n",
        "Collect Data:\n",
        "\n",
        "Modify the script to extract relevant information like title, genre, average user rating, publication date, and price.\n",
        "Save Data:\n",
        "\n",
        "Save the collected data to a CSV file for further analysis.\n",
        "Data Preprocessing:\n",
        "\n",
        "Clean the data, handle missing values, and format the variables as needed.\n",
        "Analysis:\n",
        "\n",
        "Perform statistical analysis or visualization to explore relationships between book attributes and pricing.\n",
        "Amount of Data Needed:\n",
        "\n",
        "Collect data from a sufficient number of pages to ensure a diverse sample of books across different genres, ratings, and publication dates. Aim for a few thousand samples to obtain robust insights.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBXsuppHmG_4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import bs4\n",
        "import csv\n",
        "\n",
        "csv_file = open('book_stall1.csv', 'w', newline='', encoding='utf-8')       # Opening a csv file for writing and specifying the field names\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['title', 'rating', 'cost'])\n",
        "\n",
        "base_url = 'https://books.toscrape.com/catalogue/page-{}.html'    #I have used base url  which is flexible for webscrapping\n",
        "\n",
        "sample_count = 0\n",
        "page_num = 1\n",
        "\n",
        "#Loop to continue scraping until we have 1000 samples\n",
        "while sample_count < 1000:\n",
        "    res = requests.get(base_url.format(page_num))\n",
        "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
        "\n",
        "    for article in soup.select('.product_pod'):\n",
        "        title = article.h3.a['title']\n",
        "        rating = article.p['class'][1]\n",
        "        cost = article.find('div', class_='product_price').p.text\n",
        "\n",
        "        csv_writer.writerow([title, rating, cost])\n",
        "        sample_count += 1\n",
        "\n",
        "        if sample_count >= 1000:   # Checking if we've collected 1000 samples\n",
        "            break\n",
        "\n",
        "    page_num += 1   # Moving on to the next page\n",
        "\n",
        "csv_file.close()    #closing the csv file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vJrVjjpmG_5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def collect_articles(keyword, start_year, end_year):\n",
        "\n",
        "\n",
        "  articles = []\n",
        "\n",
        "  # Construct the search query URL.\n",
        "  url = f\"https://dl.acm.org/results.cfm?query={keyword}&publication_date=({start_year},{end_year})&filtered=true\"\n",
        "\n",
        "  # Get the search results page.\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Parse the search results page.\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  # Get all the article results.\n",
        "  article_results = soup.find_all(\"div\", class_=\"article-result\")\n",
        "\n",
        "  # Iterate over the article results and extract the article information.\n",
        "  for article_result in article_results:\n",
        "    title = article_result.find(\"h4\").text\n",
        "    venue = article_result.find(\"span\", class_=\"venue\").text\n",
        "    year = article_result.find(\"span\", class_=\"year\").text\n",
        "    authors = article_result.find(\"span\", class_=\"authors\").text.split(\", \")\n",
        "    abstract = article_result.find(\"span\", class_=\"abstract\").text\n",
        "\n",
        "    # Add the article information to the list of articles.\n",
        "    articles.append({\n",
        "      \"title\": title,\n",
        "      \"venue\": venue,\n",
        "      \"year\": year,\n",
        "      \"authors\": authors,\n",
        "      \"abstract\": abstract\n",
        "    })\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Collect 1000 articles from ACM Digital Libraries with the keyword \"information retrieval\" published in the last 10 years.\n",
        "articles = collect_articles(\"information retrieval\", 2013, 2023)\n",
        "\n",
        "# Save the articles to a JSON file.\n",
        "import json\n",
        "with open(\"articles.json\", \"w\") as f:\n",
        "    json.dump(articles, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xselpl0WmG_7",
        "outputId": "dc2caa24-e883-481a-c54f-09e07e6ee747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\win_unicode_console\\__init__.py:31: RuntimeWarning: sys.stdin.encoding == 'cp1252', whereas sys.stdout.encoding == 'UTF-8', readline hook consumer may assume they are the same\n",
            "  readline_hook.enable(use_pyreadline=use_pyreadline)\n"
          ]
        }
      ],
      "source": [
        "import instaloader\n",
        "\n",
        "# Initialize the Instaloader\n",
        "loader = instaloader.Instaloader()\n",
        "\n",
        "# Define the target Instagram account\n",
        "target_username = \"alan_schaller\"\n",
        "\n",
        "# Load the profile of the target account\n",
        "target_profile = instaloader.Profile.from_username(loader.context, target_username)\n",
        "\n",
        "# Initialize a list to store the collected data\n",
        "collected_posts = []\n",
        "\n",
        "# to Collect posts\n",
        "for post in target_profile.get_posts():\n",
        "    # Get username, post_time, and post_text\n",
        "    username = post.owner_username\n",
        "    post_time = post.date\n",
        "    post_text = post.caption if post.caption else \"\"\n",
        "\n",
        "    # Append the data to the list\n",
        "    collected_posts.append([username, post_time, post_text])\n",
        "\n",
        "    # Break the loop once 1000 posts are collected\n",
        "    if len(collected_posts) >= 1000:\n",
        "        break\n",
        "\n",
        "# Save the collected data to a CSV file\n",
        "import csv\n",
        "\n",
        "csv_file_name = f'instagram_posts_{target_username}.csv'\n",
        "with open(csv_file_name, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['User Name', 'Posted Time', 'Text'])\n",
        "    csv_writer.writerows(collected_posts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjBa7Pi1mG_8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vlJrkc6mG_8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}