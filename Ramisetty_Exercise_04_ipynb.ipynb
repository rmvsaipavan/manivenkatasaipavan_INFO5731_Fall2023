{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmvsaipavan/manivenkatasaipavan_INFO5731_Fall2023/blob/main/Ramisetty_Exercise_04_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAf0cAvZ26Rj"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyzokpaV26Rl"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYF4xNOI26Rl"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj3ieotz26Rm",
        "outputId": "f3cf565a-b29a-451e-e460-7cd4a91c5392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Number of Topics: 5\n",
            "Coherence Score: 0.5772\n",
            "Topic 1:\n",
            "  Keywords:  ,    ,     ,   , s, vol, life, world, love, story\n",
            "Topic 2:\n",
            "  Keywords:  ,    ,      , s, year, new,   ,     , city, love\n",
            "Topic 3:\n",
            "  Keywords:  ,    , s,     , day, chronicle, note, death, family, book\n",
            "Topic 4:\n",
            "  Keywords:  ,    , fruit, basket,     , s, life, vol, boy, heaven\n",
            "Topic 5:\n",
            "  Keywords:  ,    , girl, secret, art, lose, work, find, love, shopaholic\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries if not already installed\n",
        "# !pip install pandas gensim spacy\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "from gensim import corpora, models\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('book_stall1.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, numbers, and special characters\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Convert to lowercase and tokenize\n",
        "    tokens = [token.lemma_ for token in nlp(text.lower()) if not token.is_stop]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "df['tokens'] = df['title'].apply(preprocess_text)\n",
        "\n",
        "# Create a dictionary and a corpus\n",
        "dictionary = corpora.Dictionary(df['tokens'])\n",
        "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
        "\n",
        "# Determine the optimal number of topics using coherence score\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list, coherence_values\n",
        "\n",
        "# Choose a range of potential number of topics (adjust as needed)\n",
        "start = 2\n",
        "limit = 12\n",
        "step = 1\n",
        "\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus, df['tokens'], limit, start, step)\n",
        "\n",
        "# Find the optimal number of topics\n",
        "optimal_num_topics = start + coherence_values.index(max(coherence_values)) * step\n",
        "\n",
        "# Train the final LDA model with the optimal number of topics\n",
        "final_lda_model = model_list[coherence_values.index(max(coherence_values))]\n",
        "\n",
        "# Extract and summarize topics\n",
        "topics = final_lda_model.show_topics(formatted=False)\n",
        "\n",
        "# ... (previous code remains the same)\n",
        "\n",
        "# Print the coherence score\n",
        "print(f'Optimal Number of Topics: {optimal_num_topics}')\n",
        "print(f'Coherence Score: {max(coherence_values):.4f}')\n",
        "\n",
        "# Extract and summarize topics\n",
        "topics = final_lda_model.show_topics(formatted=False)\n",
        "\n",
        "\n",
        "# Print out the topics\n",
        "for topic in topics:\n",
        "    print(f'Topic {topic[0]+1}:')\n",
        "    keywords = [word for word, _ in topic[1]]\n",
        "    print(f'  Keywords: {\", \".join(keywords)}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwlIIsZw26Rn"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rQaV53426Rn",
        "outputId": "15276d1a-3df4-454a-ae3f-b6c9be43e28f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in scalar divide\n",
            "  m_lr_i = np.log(numerator / denominator)\n",
            "C:\\Users\\Sai Pavan\\anaconda\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Number of Topics: 2\n",
            "Topic 1: girl life trilogy millennium ice murder guide lost good train\n",
            "Topic 2: life vol love art american world earth basket fruits story\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "\n",
        "# Step 1: Load and preprocess data\n",
        "df = pd.read_csv('book_stall1.csv')\n",
        "documents = df['title'].tolist()  # Replace 'text_column' with the actual column name containing text\n",
        "\n",
        "# Step 2: Text preprocessing (tokenization, stopword removal, lemmatization) - You can use libraries like spaCy or NLTK for this.\n",
        "\n",
        "# Step 3: Create TF-IDF matrix\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 4: Apply LSA to reduce dimensionality\n",
        "lsa_model = TruncatedSVD(n_components=300)  # Adjust n_components as needed\n",
        "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Step 5 (continued): Determine optimal number of topics (K) using coherence score\n",
        "dictionary = Dictionary([doc.split() for doc in documents])\n",
        "corpus = [dictionary.doc2bow(doc.split()) for doc in documents]\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):  # Adjust the range of K as needed\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_scores.append(coherence_model.get_coherence())\n",
        "\n",
        "optimal_k = coherence_scores.index(max(coherence_scores)) + 2  # Add 2 because we started from K=2\n",
        "\n",
        "# Print optimal number of topics and coherence score\n",
        "print(f\"Optimal Number of Topics: {optimal_k}\")\n",
        "\n",
        "# Step 6: Generate topics using LSA\n",
        "final_lsa_model = TruncatedSVD(n_components=optimal_k)\n",
        "final_lsa_matrix = final_lsa_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Step 7: Summarize topics\n",
        "terms = tfidf_vectorizer.get_feature_names_out()  # Get the terms (words)\n",
        "for i, topic in enumerate(final_lsa_model.components_):\n",
        "    topic_terms = \" \".join([terms[j] for j in topic.argsort()[:-10 - 1:-1]])  # Get top 10 terms for each topic\n",
        "    print(f\"Topic {i+1}: {topic_terms}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0UhaY5H26Ro"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S63ik2eR26Ro",
        "outputId": "32845c9c-69a1-43f8-988b-5fb38f7c0c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lda2vec in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (0.16.10)\n",
            "Requirement already satisfied: spacy in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (3.7.2)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (8.2.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (0.3.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (1.26.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (1.10.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from spacy) (61.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sai pavan\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
            "Topic #1:\n",
            "shades novel great woman vampire kitchen legend island project memoir\n",
            "Topic #2:\n",
            "story love world art trilogy true play new earth time\n",
            "Topic #3:\n",
            "vol basket volume saga collected red editions fruits fruit sandman\n",
            "Topic #4:\n",
            "harry potter dark day prince art giant games vol game\n",
            "Topic #5:\n",
            "guide thing home lose war day new tell genius wild\n",
            "Topic #6:\n",
            "life ice love death vol black search high note walk\n",
            "Topic #7:\n",
            "girl live good end raven cycle life history heart leave\n",
            "Topic #8:\n",
            "chronicles science god shadow shopaholic night vol new human mind\n",
            "Topic #9:\n",
            "book paris life run court glass world midnight city little\n",
            "Topic #10:\n",
            "secret america murder universe american grayson little complete love boy\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "!pip install lda2vec spacy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import spacy\n",
        "\n",
        "# Step 1: Load and preprocess data\n",
        "df = pd.read_csv('book_stall1.csv')\n",
        "documents = df['title'].tolist()  # Replace 'text_column' with the actual column name containing text\n",
        "\n",
        "# Step 2: Text preprocessing (tokenization, stopword removal, lemmatization)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "processed_documents = []\n",
        "for doc in documents:\n",
        "    doc = nlp(doc)\n",
        "    processed_doc = \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
        "    processed_documents.append(processed_doc)\n",
        "\n",
        "# Step 3: Create a TF-IDF matrix\n",
        "tfidf_vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words=ENGLISH_STOP_WORDS)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_documents)\n",
        "\n",
        "# Step 4: Apply LDA to generate topics\n",
        "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)  # You can adjust the number of topics\n",
        "lda_model.fit(tfidf_matrix)\n",
        "\n",
        "# Step 5: Summarize topics\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic #{topic_idx+1}:\")\n",
        "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3e9j-Fw26Ro"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npTWoG_T3Lhf",
        "outputId": "c379a4e3-39d9-4364-9eda-76af512f4231"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m642.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.23.5)\n",
            "Collecting hdbscan>=0.8.29 (from bertopic)\n",
            "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn>=0.5.0 (from bertopic)\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.1)\n",
            "Collecting sentence-transformers>=0.4.1 (from bertopic)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.15.0)\n",
            "Collecting cython<3,>=0.27 (from hdbscan>=0.8.29->bertopic)\n",
            "  Using cached Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (23.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.16.0+cu118)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn>=0.5.0->bertopic)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tbb>=2019.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (2021.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.7)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.4.1->bertopic)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=3039150 sha256=a58ed98375b15c31c06e0ffe71587c48ab16226f5c6030c0a3212de347454d89\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=4d981b18d53160ecdeae9a5300dc7f76a64433c447201fe0c661a5cc087b4ba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86770 sha256=263160286d43863dd4872237393e5302c75df8f457b4522ffcdc34f3545a2308\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=e8cfb9708d22e067dfb502851bab515142ab30435c212fc6b1610dc63350233f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
            "Installing collected packages: sentencepiece, safetensors, cython, huggingface-hub, tokenizers, pynndescent, hdbscan, umap-learn, transformers, sentence-transformers, bertopic\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.4\n",
            "    Uninstalling Cython-3.0.4:\n",
            "      Successfully uninstalled Cython-3.0.4\n",
            "Successfully installed bertopic-0.15.0 cython-0.29.36 hdbscan-0.8.33 huggingface-hub-0.17.3 pynndescent-0.5.10 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.35.0 umap-learn-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IU6JIp4U26Ro"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv(\"book_stall1.csv\")\n",
        "text_data = data['title'].astype(str).tolist()\n",
        "\n",
        "# Initialize BERTopic\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
        "\n",
        "# Fit the model to the data\n",
        "topics, _ = topic_model.fit_transform(text_data)\n",
        "\n",
        "# Calculate coherence scores for different numbers of topics\n",
        "coherence_scores = []\n",
        "range_of_topics = range(2, 21)  # Adjust the range as needed\n",
        "\n",
        "for k in range_of_topics:\n",
        "    model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, nr_topics=k)\n",
        "    topics, _ = model.fit_transform(text_data)\n",
        "    coherence_score = model.get_topic_info().coherence_mean.mean()\n",
        "    coherence_scores.append(coherence_score)\n",
        "\n",
        "# Determine the optimal number of topics with the highest coherence score\n",
        "best_k = range_of_topics[coherence_scores.index(max(coherence_scores))]\n",
        "best_coherence_score = max(coherence_scores)\n",
        "\n",
        "print(f\"The best number of topics is {best_k} with a coherence score of {best_coherence_score}\")\n",
        "\n",
        "# Train the final BERTopic model with the optimal number of topics\n",
        "model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True, nr_topics=best_k)\n",
        "topics, _ = model.fit_transform(text_data)\n",
        "\n",
        "# Summarize the topics\n",
        "most_frequent_topics = model.get_topic_info()\n",
        "most_frequent_topics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrpdBXxX7ARL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpwwWFHL26Rp"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9QxBfoh26Rp"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\"\"\"\n",
        "LSA modeling algorithm is more efficient than the other three algorithms.\n",
        "The statement that \"LSA modeling algorithm is more efficient than the other three algorithms (LDA, lda2vec, and BERTopic)\n",
        "when dealing with large datasets\" can be supported based on the following reasons:\n",
        "\n",
        "1. Dimensionality Reduction: LSA is primarily a dimensionality reduction technique that uses Singular Value Decomposition (SVD)\n",
        "to reduce the dimensionality of the document-term matrix. By reducing the number of dimensions, LSA can handle large datasets\n",
        "more efficiently. This reduction in dimensionality helps in reducing the computational complexity and memory requirements,\n",
        "making LSA more suitable for large datasets.\n",
        "\n",
        "2. Computational Efficiency: LSA performs SVD on the document-term matrix, which has a time complexity of O(n^2m), where n is\n",
        "the number of documents and m is the number of unique terms. However, the SVD operation can be efficiently implemented using\n",
        "optimized libraries like SciPy or NumPy, which further improves the computational efficiency of LSA. In contrast, algorithms\n",
        "like LDA, lda2vec, and BERTopic may require more computational resources and training time due to their complex architectures\n",
        "and larger model sizes.\n",
        "\n",
        "3. Scalability: LSA can handle large datasets by processing them in batches or by using distributed computing frameworks like\n",
        "Spark. This scalability allows LSA to efficiently process and analyze large volumes of text data. On the other hand, algorithms\n",
        "like LDA, lda2vec, and BERTopic may face challenges in terms of memory requirements and training time when dealing with large\n",
        "datasets.\n",
        "\n",
        "4. Trade-off with Interpretability: While LSA is efficient in handling large datasets, it may sacrifice some interpretability\n",
        "compared to algorithms like LDA. LSA focuses on capturing latent semantic relationships and reducing noise, but it may not\n",
        "provide explicit topic-word distributions or document-topic proportions.If interpretability is crucial, LDA may be a superior\n",
        "option, even if it is less efficient for huge datasets.\n",
        "\n",
        "In summary, due to its dimensionality reduction capabilities, computational efficiency, scalability, and capacity to handle\n",
        "enormous volumes of text data, LSA is regarded more efficient than LDA, lda2vec, and BERTopic when dealing with large datasets.\n",
        "However, before selecting the best algorithm, consider the trade-off with interpretability and examine the task's specific\n",
        "requirements.\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}